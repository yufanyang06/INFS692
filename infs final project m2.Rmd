---
title: "INFS 692 Final Project M2"
output: pdf_document
date: "2022-12-16"
---
## Data entry
```{r}
library(readr)
df = read.csv("radiomics_completedata.csv")
```

## Packages
```{r}
library(dplyr)  
library(keras)
library(caret)
library(rsample)   
library(recipes)
library(tfruns) 
```

## Data prepreation 
```{r }
summary(df)

```
```{r}
df <- na.omit(df)
```

## split data (70% and 30%)
```{r}
index<-createDataPartition(df$Failure.binary,p=0.7,list=F)

X <- data.matrix(df[index,-2])
Y <- df[index,2]
test_X <- data.matrix(df[-index,-2])
test_Y <- df[-index,2]
```

## binary conversion
```{r}
as.matrix(apply(X, 2, function(x) (x-min(x))/(max(x) - min(x)))) ->
  X

as.matrix(apply(test_X, 2, function(x) (x-min(x))/(max(x) - min(x)))) -> 
  test_X

to_categorical(Y, num_classes = 2) -> Y
to_categorical(test_Y, num_classes = 2) -> test_Y
```

## Create five hidden layers with 256, 128, 128, 64 and 64 neurons, respectively with activation functions of Sigmoid. Create an output layer with two neurons respectively with activation functions of Softmax. Every layer is followed by a dropout to avoid overfitting.
```{r}
model <- keras_model_sequential() %>%
  layer_dense(units = 256, activation = "sigmoid", input_shape=ncol(X))%>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 128, activation = "sigmoid") %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 128, activation = "sigmoid") %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 64, activation = "sigmoid") %>% 
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 64, activation = "sigmoid") %>% 
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 2, activation = "sigmoid") 
```
## Backpropagation
```{r}
model %>% compile(
    loss = "categorical_crossentropy",
    optimizer = optimizer_rmsprop(),
    metrics = c("accuracy")
  )
```
## Compiler approach
```{r}
model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_adam(),
  metrics = c("accuracy")
)
```
## Train the model with epoch = 10, batch size = 128 and validation split = 0.15
```{r}
history <- model %>% 
  fit(X, Y, epochs = 10, batch_size = 128, validation_split = 0.15)
```
## Model evaluation
```{r}
model %>%
  evaluate(test_X, test_Y)
```
## Model prediction
```{r}
model %>%
  predict(test_X)
```


